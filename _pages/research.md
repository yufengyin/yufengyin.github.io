---
layout: archive
title: ""
permalink: /research/
author_profile: true
redirect_from:
  - /research
---

Research Experience
======
**University of Southern California Intelligent Human Perception Lab**, 2019 - now

* Advisor: Prof. [Mohammad Soleymani](https://www.ihp-lab.org/)

* *Speaker-Invariant Adversarial Domain Adaptation for Emotion Recognition* [[pdf]](https://yufengyin.github.io/files/icmi20.pdf), [[code]](https://github.com/intelligent-human-perception-laboratory/Speaker-Invariant-Domain-Adversarial-Neural-Networks), 2020
  * Studied the unsupervised domain adaptation problem on emotion recogintion with multimodal data
  * Proposed Speaker-Invariant Domain-Adversarial Neural Network to reduce both the domain bias and the speaker bias
  * Authored a paper published in *ICMI'20*

**Tsinghua University Human-Computer Speech Interaction Research Group**, 2016 - 2019

* Advisor: Prof. [Jia Jia](https://hcsi.cs.tsinghua.edu.cn/)

* *Understanding the Teaching Styles by an Attention based Multi-task Cross-media Dimensional Modeling* [[pdf]](https://yufengyin.github.io/files/mm19.pdf), 2018 to 2019
  * Established a fully-annotated voice data set (4,451 utterances) with pleasure and arousal values
  * Created a two-dimensional Teaching Style Semantic Space (TSSS) to determine teachersâ€™ teaching styles
  * Proposed a multi-task cross-media model to map acoustic features to coordinates on the TSSS
  * Co-authored a paper published in *ACM MM'19*

* *Inferring Emotions from Large-scale Internet Voice Data*  [[pdf]](https://yufengyin.github.io/files/tmm19.pdf), 2017 to 2018
  * Employed DNN and LSTM with autoencoders to infer emotions from large-scale internet voice data
  * Processed data, created neural networks, conducted experiments and edited the paper
  * Co-authored a paper published in *TMM'19*

* *Inferring Emotion from Conversational Voice Data: A Semi-supervised Multi-path Generative Neural Network Approach* [[pdf]](https://yufengyin.github.io/files/aaai18.pdf), 2017
  * Proposed a novel model to infer emotion from conversational voice data
  * Collected over 24,000 real-world utterance, processed data and edited paper
  * Co-authored a paper published in *AAAI'18*

**Stanford University Human-Computer Interaction Research Group**, 2018

* Advisors: Prof. [James Landay](https://profiles.stanford.edu/james-landay), Prof. [Emma Brunskill](https://cs.stanford.edu/people/ebrun/)

* *The Smart Primer* [[link](https://hci.stanford.edu/research/smartprimer/projects/smartprimer.html), [poster](https://yufengyin.github.io/files/poster.pdf), [slides](https://yufengyin.github.io/files/slides.pdf)], 2018
  * A personal tutor for children that uses narrative and embedded physical world activities to enhance learning
  * *Stanford University Undergraduate Visiting Research Program (UGVR)*
  * Created a chat bot and a quiz bot to guide users
  * Worked as the architect of the whole project for both frontend and backend coding
